import { useCallback, useEffect, useRef, useState } from "react";
import { Client } from "@stomp/stompjs";
import SockJS from "sockjs-client";
import { VoiceCallStatus, VoiceSignalType } from "../types/voiceChat";
import type {
  VoiceModulationSettings,
  VoiceSignalMessage,
} from "../types/voiceChat";
import { voiceChatService } from "../api/voiceChatService";

interface UseVoiceChatProps {
  chatRoomId: number;
  myProfileId: number;
  partnerProfileId: number;
}

export const useVoiceChat = ({
  chatRoomId,
  myProfileId,
  partnerProfileId,
}: UseVoiceChatProps) => {
  const [callStatus, setCallStatus] = useState<VoiceCallStatus>(
    VoiceCallStatus.IDLE
  );
  const [isConnected, setIsConnected] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeakerOn, setIsSpeakerOn] = useState(true);
  const [callDuration, setCallDuration] = useState(0);
  const [modulationSettings, setModulationSettings] =
    useState<VoiceModulationSettings>({
      pitch: 1.0,
      tempo: 1.0,
      rate: 1.0,
      effectType: "none",
    });

  const stompClientRef = useRef<Client | null>(null);
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null);
  const localStreamRef = useRef<MediaStream | null>(null);
  const remoteStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  // const soundTouchRef = useRef<any | null>(null);
  const callTimerRef = useRef<number | null>(null);
  const callStartTimeRef = useRef<Date | null>(null); // í†µí™” ì‹œì‘ ì‹œê° ì €ì¥
  const handleSignalMessageRef = useRef<
    ((signal: VoiceSignalMessage) => Promise<void>) | null
  >(null);

  // WebSocket ì—°ê²°
  const connectWebSocket = useCallback(() => {
    const token = localStorage.getItem("accessToken");
    const socket = new SockJS(`${import.meta.env.VITE_API_BASE_URL}/ws`);
    const client = new Client({
      webSocketFactory: () => socket,
      connectHeaders: {
        Authorization: `Bearer ${token}`,
      },
      debug: (str) => {
        console.log("[STOMP Debug]", str);
      },
      onConnect: () => {
        console.log("ğŸ”Œ [WebSocket] ì—°ê²° ì„±ê³µ");
        setIsConnected(true);

        // ì±„íŒ…ë°© í† í”½ êµ¬ë… (ë¸Œë¡œë“œìºìŠ¤íŠ¸ ìˆ˜ì‹ )
        client.subscribe(`/topic/voice.${chatRoomId}`, (message) => {
          const signal: VoiceSignalMessage = JSON.parse(message.body);
          console.log("ğŸ“¡ [ì±„íŒ…ë°© ë¸Œë¡œë“œìºìŠ¤íŠ¸] ë©”ì‹œì§€ ìˆ˜ì‹ :", {
            type: signal.type,
            senderId: signal.senderId,
            myProfileId,
            willProcess: signal.senderId !== myProfileId,
          });

          // â­ ìì‹ ì´ ë³´ë‚¸ ë©”ì‹œì§€ëŠ” ë¬´ì‹œ (echo ë°©ì§€)
          if (signal.senderId !== myProfileId) {
            console.log("âœ… [ì²˜ë¦¬] ìƒëŒ€ë°© ë©”ì‹œì§€ ì²˜ë¦¬:", signal.type);
            if (handleSignalMessageRef.current) {
              handleSignalMessageRef.current(signal);
            }
          } else {
            console.log("â­ï¸ [ë¬´ì‹œ] ë‚´ê°€ ë³´ë‚¸ ë©”ì‹œì§€ echo");
          }
        });
      },
      onStompError: (frame) => {
        console.error("[WebSocket] ì˜¤ë¥˜:", frame);
        setIsConnected(false);
      },
    });

    client.activate();
    stompClientRef.current = client;
  }, [myProfileId, chatRoomId]);

  // ì‹œê·¸ë„ ë©”ì‹œì§€ ì „ì†¡
  const sendSignal = useCallback(
    (type: VoiceSignalType, extraData?: Partial<VoiceSignalMessage>) => {
      if (!stompClientRef.current?.connected) {
        console.error("[WebSocket] ì—°ê²°ë˜ì§€ ì•ŠìŒ");
        return;
      }

      const message: VoiceSignalMessage = {
        type,
        chatRoomId,
        fromProfileId: myProfileId,
        toProfileId: partnerProfileId,
        senderId: myProfileId,
        timestamp: Date.now(),
        ...extraData,
      };

      stompClientRef.current.publish({
        destination: `/app/voice/${chatRoomId}`,
        body: JSON.stringify(message),
      });

      console.log("[Signal] ì „ì†¡:", type, "to chatRoom:", chatRoomId);
    },
    [chatRoomId, myProfileId, partnerProfileId]
  );

  // ìŒì„± ë³€ì¡° ì„¤ì •
  const applyVoiceModulation = useCallback(
    (stream: MediaStream): MediaStream => {
      if (!audioContextRef.current) {
        audioContextRef.current = new AudioContext();
      }

      const audioContext = audioContextRef.current;
      const source = audioContext.createMediaStreamSource(stream);
      const destination = audioContext.createMediaStreamDestination();

      // Web Audio APIë¥¼ ì‚¬ìš©í•œ pitch/tempo ë³€ì¡°
      // ì‹¤ì œ pitch ë³€ê²½ì€ ë³µì¡í•˜ë¯€ë¡œ í•„í„°ë§Œ ì ìš©

      // íš¨ê³¼ ì ìš©
      switch (modulationSettings.effectType) {
        case "robot": {
          const filter = audioContext.createBiquadFilter();
          filter.type = "bandpass";
          filter.frequency.value = 1000;
          filter.Q.value = 10;
          source.connect(filter);
          filter.connect(destination);
          break;
        }
        case "echo": {
          const delay = audioContext.createDelay();
          delay.delayTime.value = 0.3;
          const feedback = audioContext.createGain();
          feedback.gain.value = 0.4;

          source.connect(destination);
          source.connect(delay);
          delay.connect(feedback);
          feedback.connect(delay);
          feedback.connect(destination);
          break;
        }
        case "deep": {
          const filter = audioContext.createBiquadFilter();
          filter.type = "lowpass";
          filter.frequency.value = 500;
          source.connect(filter);
          filter.connect(destination);
          break;
        }
        case "high": {
          const filter = audioContext.createBiquadFilter();
          filter.type = "highpass";
          filter.frequency.value = 2000;
          source.connect(filter);
          filter.connect(destination);
          break;
        }
        default:
          source.connect(destination);
      }

      return destination.stream;
    },
    [modulationSettings]
  );

  // WebRTC ì—°ê²° ìƒì„±
  const createPeerConnection = useCallback(() => {
    const configuration: RTCConfiguration = {
      iceServers: [
        { urls: "stun:stun.l.google.com:19302" },
        { urls: "stun:stun1.l.google.com:19302" },
      ],
    };

    const pc = new RTCPeerConnection(configuration);

    pc.onicecandidate = (event) => {
      if (event.candidate) {
        sendSignal(VoiceSignalType.ICE_CANDIDATE, {
          candidate: event.candidate.toJSON(),
        });
      }
    };

    pc.ontrack = (event) => {
      console.log("[WebRTC] ì›ê²© ìŠ¤íŠ¸ë¦¼ ìˆ˜ì‹ ");
      remoteStreamRef.current = event.streams[0];
      setCallStatus(VoiceCallStatus.CONNECTED);

      // í†µí™” ì‹œì‘ ì‹œê° ê¸°ë¡
      if (!callStartTimeRef.current) {
        callStartTimeRef.current = new Date();
      }

      if (callTimerRef.current) {
        clearInterval(callTimerRef.current);
      }
      setCallDuration(0);
      callTimerRef.current = setInterval(() => {
        setCallDuration((prev) => prev + 1);
      }, 1000) as unknown as number;
    };

    pc.onconnectionstatechange = () => {
      console.log("[WebRTC] ì—°ê²° ìƒíƒœ:", pc.connectionState);
      if (
        pc.connectionState === "disconnected" ||
        pc.connectionState === "failed"
      ) {
        // endCall ëŒ€ì‹  cleanup ì§ì ‘ í˜¸ì¶œ
        if (callTimerRef.current) {
          clearInterval(callTimerRef.current);
          callTimerRef.current = null;
        }
        if (localStreamRef.current) {
          localStreamRef.current.getTracks().forEach((track) => track.stop());
          localStreamRef.current = null;
        }
        if (peerConnectionRef.current) {
          peerConnectionRef.current.close();
          peerConnectionRef.current = null;
        }
        if (audioContextRef.current) {
          audioContextRef.current.close();
          audioContextRef.current = null;
        }
        setCallDuration(0);
        setCallStatus(VoiceCallStatus.ENDED);
      }
    };

    peerConnectionRef.current = pc;
    return pc;
  }, [sendSignal]);

  // ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ê°€ì ¸ì˜¤ê¸°
  const getLocalStream = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      localStreamRef.current = stream;

      // ìŒì„± ë³€ì¡° ì ìš©
      const modulatedStream = applyVoiceModulation(stream);

      return modulatedStream;
    } catch (error) {
      console.error("[ë§ˆì´í¬] ì ‘ê·¼ ì˜¤ë¥˜:", error);
      throw error;
    }
  }, [applyVoiceModulation]);

  // ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  const cleanup = useCallback(() => {
    if (callTimerRef.current) {
      clearInterval(callTimerRef.current);
      callTimerRef.current = null;
    }

    if (localStreamRef.current) {
      localStreamRef.current.getTracks().forEach((track) => track.stop());
      localStreamRef.current = null;
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.close();
      peerConnectionRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    setCallDuration(0);
  }, []);

  // ì‹œê·¸ë„ ë©”ì‹œì§€ ì²˜ë¦¬
  const handleSignalMessage = useCallback(
    async (signal: VoiceSignalMessage) => {
      console.log("[Signal] ìˆ˜ì‹ :", signal.type);

      switch (signal.type) {
        case VoiceSignalType.VOICE_CALL_REQUEST:
          setCallStatus(VoiceCallStatus.INCOMING);
          break;

        case VoiceSignalType.VOICE_CALL_ACCEPT:
          setCallStatus(VoiceCallStatus.CONNECTING);
          // Offer ìƒì„±
          try {
            const pc = createPeerConnection();
            const stream = await getLocalStream();
            stream.getTracks().forEach((track) => {
              pc.addTrack(track, stream);
            });

            const offer = await pc.createOffer();
            await pc.setLocalDescription(offer);

            sendSignal(VoiceSignalType.OFFER, {
              sdp: offer.sdp,
            });
          } catch (error) {
            console.error("[WebRTC] Offer ìƒì„± ì˜¤ë¥˜:", error);
          }
          break;

        case VoiceSignalType.VOICE_CALL_REJECT:
          setCallStatus(VoiceCallStatus.ENDED);
          cleanup();
          break;

        case VoiceSignalType.VOICE_CALL_END:
          setCallStatus(VoiceCallStatus.ENDED);
          cleanup();
          break;

        case VoiceSignalType.OFFER:
          try {
            const pc = createPeerConnection();
            const stream = await getLocalStream();
            stream.getTracks().forEach((track) => {
              pc.addTrack(track, stream);
            });

            await pc.setRemoteDescription(
              new RTCSessionDescription({
                type: "offer",
                sdp: signal.sdp,
              })
            );

            const answer = await pc.createAnswer();
            await pc.setLocalDescription(answer);

            sendSignal(VoiceSignalType.ANSWER, {
              sdp: answer.sdp,
            });
          } catch (error) {
            console.error("[WebRTC] Answer ìƒì„± ì˜¤ë¥˜:", error);
          }
          break;

        case VoiceSignalType.ANSWER:
          try {
            if (peerConnectionRef.current) {
              await peerConnectionRef.current.setRemoteDescription(
                new RTCSessionDescription({
                  type: "answer",
                  sdp: signal.sdp,
                })
              );
            }
          } catch (error) {
            console.error("[WebRTC] Answer ì„¤ì • ì˜¤ë¥˜:", error);
          }
          break;

        case VoiceSignalType.ICE_CANDIDATE:
          try {
            if (peerConnectionRef.current && signal.candidate) {
              await peerConnectionRef.current.addIceCandidate(
                new RTCIceCandidate(signal.candidate)
              );
            }
          } catch (error) {
            console.error("[WebRTC] ICE Candidate ì¶”ê°€ ì˜¤ë¥˜:", error);
          }
          break;
      }
    },
    [createPeerConnection, getLocalStream, sendSignal, cleanup]
  );

  // í†µí™” ì‹œì‘
  const startCall = useCallback(() => {
    setCallStatus(VoiceCallStatus.REQUESTING);
    // í†µí™” ì‹œì‘ ì‹œê°ì€ WebRTC ì—°ê²° ì„±ê³µ ì‹œ(ontrack)ì— ê¸°ë¡
    sendSignal(VoiceSignalType.VOICE_CALL_REQUEST);
  }, [sendSignal]);

  // í†µí™” ìˆ˜ë½
  const acceptCall = useCallback(() => {
    setCallStatus(VoiceCallStatus.CONNECTING);
    // í†µí™” ì‹œì‘ ì‹œê°ì€ WebRTC ì—°ê²° ì„±ê³µ ì‹œ(ontrack)ì— ê¸°ë¡
    sendSignal(VoiceSignalType.VOICE_CALL_ACCEPT);
  }, [sendSignal]);

  // í†µí™” ê±°ì ˆ
  const rejectCall = useCallback(() => {
    setCallStatus(VoiceCallStatus.ENDED);
    sendSignal(VoiceSignalType.VOICE_CALL_REJECT);
    // ê±°ì ˆ ì‹œì—ëŠ” ì—°ê²°ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ í†µí™” ê¸°ë¡ ì €ì¥í•˜ì§€ ì•ŠìŒ
    cleanup();
  }, [sendSignal, cleanup]);

  // í†µí™” ì¢…ë£Œ
  const endCall = useCallback(async () => {
    sendSignal(VoiceSignalType.VOICE_CALL_END);
    setCallStatus(VoiceCallStatus.ENDED);

    // í†µí™” ê¸°ë¡ ì €ì¥
    if (callStartTimeRef.current) {
      try {
        const endTime = new Date();
        const duration = Math.floor(
          (endTime.getTime() - callStartTimeRef.current.getTime()) / 1000
        );

        await voiceChatService.saveCallHistory({
          chatRoomId,
          callerProfileId: myProfileId,
          receiverProfileId: partnerProfileId,
          startTime: callStartTimeRef.current.toISOString(),
          endTime: endTime.toISOString(),
          duration: duration,
          callStatus: "COMPLETED",
          voiceModulationUsed: true, // í•­ìƒ ìŒì„± ë³€ì¡° ì‚¬ìš©
        });

        console.log(`[í†µí™” ê¸°ë¡] ì €ì¥ ì™„ë£Œ - ${duration}ì´ˆ`);
      } catch (error) {
        console.error("[í†µí™” ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨]", error);
      }
    }

    cleanup();
  }, [
    sendSignal,
    cleanup,
    chatRoomId,
    myProfileId,
    partnerProfileId,
    modulationSettings,
  ]);

  // ìŒì†Œê±° í† ê¸€
  const toggleMute = useCallback(async () => {
    if (localStreamRef.current) {
      const audioTrack = localStreamRef.current.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.enabled = !audioTrack.enabled;
        const newMutedState = !audioTrack.enabled;
        setIsMuted(newMutedState);

        // ì„œë²„ì— ìŒì†Œê±° ìƒíƒœ ì „ì†¡
        try {
          await voiceChatService.updateMuteStatus(chatRoomId, {
            profileId: myProfileId,
            isMuted: newMutedState,
            timestamp: Date.now(),
          });

          // WebSocketìœ¼ë¡œ ìƒëŒ€ë°©ì—ê²Œ ìŒì†Œê±° ìƒíƒœ ì•Œë¦¼
          sendSignal(VoiceSignalType.MUTE_STATUS_CHANGED, {
            isMuted: newMutedState,
          });
        } catch (error) {
          console.error("[ìŒì†Œê±° ìƒíƒœ ì „ì†¡ ì‹¤íŒ¨]", error);
        }
      }
    }
  }, [chatRoomId, myProfileId, sendSignal]);

  // ìŠ¤í”¼ì»¤ í† ê¸€
  const toggleSpeaker = useCallback(() => {
    setIsSpeakerOn((prev) => !prev);
  }, []);

  // ìŒì„± ë³€ì¡° ì„¤ì • ë³€ê²½
  const updateModulationSettings = useCallback(
    (settings: Partial<VoiceModulationSettings>) => {
      setModulationSettings((prev) => ({ ...prev, ...settings }));

      // í˜„ì¬ í†µí™” ì¤‘ì´ë©´ ìŠ¤íŠ¸ë¦¼ ì¬ìƒì„±
      if (callStatus === VoiceCallStatus.CONNECTED && localStreamRef.current) {
        const modulatedStream = applyVoiceModulation(localStreamRef.current);

        // ê¸°ì¡´ íŠ¸ë™ ì œê±° ë° ìƒˆ íŠ¸ë™ ì¶”ê°€
        if (peerConnectionRef.current) {
          const senders = peerConnectionRef.current.getSenders();
          senders.forEach((sender) => {
            if (sender.track?.kind === "audio") {
              const newTrack = modulatedStream.getAudioTracks()[0];
              sender.replaceTrack(newTrack);
            }
          });
        }
      }
    },
    [callStatus, applyVoiceModulation]
  );

  // ì›ê²© ì˜¤ë””ì˜¤ ì¬ìƒ
  const getRemoteAudio = useCallback(() => {
    if (remoteStreamRef.current) {
      const audio = new Audio();
      audio.srcObject = remoteStreamRef.current;
      audio.volume = isSpeakerOn ? 1.0 : 0;
      audio.play();
      return audio;
    }
    return null;
  }, [isSpeakerOn]);

  // ì´ˆê¸°í™”
  useEffect(() => {
    // handleSignalMessageë¥¼ refì— í• ë‹¹
    handleSignalMessageRef.current = handleSignalMessage;
  }, [handleSignalMessage]);

  useEffect(() => {
    connectWebSocket();

    return () => {
      cleanup();
      if (stompClientRef.current) {
        stompClientRef.current.deactivate();
      }
    };
  }, [connectWebSocket, cleanup]);

  // ìŠ¤í”¼ì»¤ ë³¼ë¥¨ ì¡°ì ˆ
  useEffect(() => {
    const audio = getRemoteAudio();
    return () => {
      if (audio) {
        audio.pause();
        audio.srcObject = null;
      }
    };
  }, [getRemoteAudio]);

  return {
    callStatus,
    isConnected,
    isMuted,
    isSpeakerOn,
    callDuration,
    modulationSettings,
    startCall,
    acceptCall,
    rejectCall,
    endCall,
    toggleMute,
    toggleSpeaker,
    updateModulationSettings,
  };
};
